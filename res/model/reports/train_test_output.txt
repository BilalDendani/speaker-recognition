(speaker-recognition) ✔ ~/work/projects/speaker-recognition/code [master|⚑ 1] 
09:43 $ python cnn_train.py 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 20, 196, 16)       160       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 10, 98, 16)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 10, 98, 16)        64        
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 10, 98, 32)        4640      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 49, 32)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 5, 49, 32)         128       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 5, 49, 64)         18496     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 24, 64)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 2, 24, 64)         256       
_________________________________________________________________
flatten_1 (Flatten)          (None, 3072)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 3072)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2432)              7473536   
_________________________________________________________________
batch_normalization_4 (Batch (None, 2432)              9728      
_________________________________________________________________
dropout_2 (Dropout)          (None, 2432)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 1216)              2958528   
=================================================================
Total params: 10,465,536
Trainable params: 10,460,448
Non-trainable params: 5,088
_________________________________________________________________
None
Train on 58854 samples, validate on 3270 samples
Epoch 1/32
58854/58854 [==============================] - 480s 8ms/step - loss: 3.7762 - acc: 0.7402 - val_loss: 2.7140 - val_acc: 0.8202
Epoch 2/32
58854/58854 [==============================] - 519s 9ms/step - loss: 2.2019 - acc: 0.9013 - val_loss: 2.4314 - val_acc: 0.8670
Epoch 3/32
58854/58854 [==============================] - 531s 9ms/step - loss: 2.1113 - acc: 0.9232 - val_loss: 2.3802 - val_acc: 0.8988
Epoch 4/32
58854/58854 [==============================] - 556s 9ms/step - loss: 2.1404 - acc: 0.9304 - val_loss: 2.4558 - val_acc: 0.8804
Epoch 5/32
58854/58854 [==============================] - 577s 10ms/step - loss: 2.1202 - acc: 0.9365 - val_loss: 2.2491 - val_acc: 0.9257
Epoch 6/32
58854/58854 [==============================] - 579s 10ms/step - loss: 2.0210 - acc: 0.9437 - val_loss: 2.2776 - val_acc: 0.9257
Epoch 7/32
58854/58854 [==============================] - 577s 10ms/step - loss: 2.0337 - acc: 0.9437 - val_loss: 2.1751 - val_acc: 0.9232
Epoch 8/32
58854/58854 [==============================] - 587s 10ms/step - loss: 1.9057 - acc: 0.9494 - val_loss: 2.0650 - val_acc: 0.9355
Epoch 9/32
58854/58854 [==============================] - 562s 10ms/step - loss: 1.8742 - acc: 0.9519 - val_loss: 1.9940 - val_acc: 0.9333
Epoch 10/32
58854/58854 [==============================] - 493s 8ms/step - loss: 1.7736 - acc: 0.9549 - val_loss: 1.9346 - val_acc: 0.9419
Epoch 11/32
58854/58854 [==============================] - 503s 9ms/step - loss: 1.7385 - acc: 0.9560 - val_loss: 1.7791 - val_acc: 0.9462
Epoch 12/32
58854/58854 [==============================] - 485s 8ms/step - loss: 1.6619 - acc: 0.9577 - val_loss: 1.7507 - val_acc: 0.9495
Epoch 13/32
58854/58854 [==============================] - 498s 8ms/step - loss: 1.5819 - acc: 0.9610 - val_loss: 1.5891 - val_acc: 0.9511
Epoch 14/32
58854/58854 [==============================] - 476s 8ms/step - loss: 1.5459 - acc: 0.9611 - val_loss: 1.5581 - val_acc: 0.9538
Epoch 15/32
58854/58854 [==============================] - 541s 9ms/step - loss: 1.4704 - acc: 0.9628 - val_loss: 1.5578 - val_acc: 0.9538
Epoch 16/32
58854/58854 [==============================] - 522s 9ms/step - loss: 1.4164 - acc: 0.9640 - val_loss: 1.5677 - val_acc: 0.9575
Epoch 17/32
58854/58854 [==============================] - 468s 8ms/step - loss: 1.3530 - acc: 0.9657 - val_loss: 1.4309 - val_acc: 0.9606
Epoch 18/32
58854/58854 [==============================] - 470s 8ms/step - loss: 1.2976 - acc: 0.9662 - val_loss: 1.4731 - val_acc: 0.9502
Epoch 19/32
58854/58854 [==============================] - 469s 8ms/step - loss: 1.2306 - acc: 0.9675 - val_loss: 1.3384 - val_acc: 0.9538
Epoch 20/32
58854/58854 [==============================] - 470s 8ms/step - loss: 1.2117 - acc: 0.9669 - val_loss: 1.2420 - val_acc: 0.9627
Epoch 21/32
58854/58854 [==============================] - 524s 9ms/step - loss: 1.1071 - acc: 0.9700 - val_loss: 1.1858 - val_acc: 0.9654
Epoch 22/32
58854/58854 [==============================] - 536s 9ms/step - loss: 1.1218 - acc: 0.9687 - val_loss: 1.1968 - val_acc: 0.9657
Epoch 23/32
58854/58854 [==============================] - 587s 10ms/step - loss: 1.0451 - acc: 0.9715 - val_loss: 1.1711 - val_acc: 0.9587
Epoch 24/32
58854/58854 [==============================] - 468s 8ms/step - loss: 0.9942 - acc: 0.9723 - val_loss: 0.9907 - val_acc: 0.9654
Epoch 25/32
58854/58854 [==============================] - 470s 8ms/step - loss: 0.9438 - acc: 0.9736 - val_loss: 1.0004 - val_acc: 0.9706
Epoch 26/32
58854/58854 [==============================] - 470s 8ms/step - loss: 0.9323 - acc: 0.9728 - val_loss: 0.9995 - val_acc: 0.9654
Epoch 27/32
58854/58854 [==============================] - 468s 8ms/step - loss: 0.9016 - acc: 0.9733 - val_loss: 0.9255 - val_acc: 0.9694
Epoch 28/32
58854/58854 [==============================] - 468s 8ms/step - loss: 0.8481 - acc: 0.9756 - val_loss: 0.9691 - val_acc: 0.9651
Epoch 29/32
58854/58854 [==============================] - 635s 11ms/step - loss: 0.8427 - acc: 0.9747 - val_loss: 0.8874 - val_acc: 0.9716
Epoch 30/32
58854/58854 [==============================] - 598s 10ms/step - loss: 0.8153 - acc: 0.9754 - val_loss: 0.8958 - val_acc: 0.9642
Epoch 31/32
58854/58854 [==============================] - 470s 8ms/step - loss: 0.7817 - acc: 0.9769 - val_loss: 0.7836 - val_acc: 0.9725
Epoch 32/32
58854/58854 [==============================] - 482s 8ms/step - loss: 0.7626 - acc: 0.9771 - val_loss: 0.8105 - val_acc: 0.9697
Saved model to disk
(speaker-recognition) ✔ ~/work/projects/speaker-recognition/code [master|✚ 2⚑ 1] 
14:19 $ python cnn_test.py 
Loaded model from disk
3270/3270 [==============================] - 8s 2ms/step
acc: 96.33%
Audio file abhishek-b0174.wav, predicted speaker: abhishek
Audio file arun-b0176.wav, predicted speaker: arun
Audio file babe-b0403.wav, predicted speaker: bebe
Audio file soulphox-b0049.wav, predicted speaker: soulphox
Audio file BlindPilot-a0276.wav, predicted speaker: BlindPilot
